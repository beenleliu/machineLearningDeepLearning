{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNH7v/JpWjIzLNFHz4h0A9X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beenleliu/machineLearningDeepLearning/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9IQeJ49Izy1"
      },
      "source": [
        "与之前介绍的多层感知机和能有效处理空间信息的卷积神经网络不同，循环神经网络是为更好地处理时序信息而设计的。它引入状态变量来存储过去的信息，并用其与当前的输入共同决定当前的输出。\n",
        "\n",
        "循环神经网络常用于处理序列数据，如一段文字或声音、购物或观影的顺序，甚至是图像中的一行或一列像素。因此，循环神经网络有着极为广泛的实际应用，如语言模型、文本分类、机器翻译、语音识别、图像分析、手写识别和推荐系统。\n",
        "\n",
        "因为本章中的应用是基于语言模型的，所以我们将先介绍语言模型的基本概念，并由此激发循环神经网络的设计灵感。接着，我们将描述循环神经网络中的梯度计算方法，从而探究循环神经网络训练可能存在的问题。对于其中的部分问题，我们可以使用本章稍后介绍的含门控的循环神经网络来解决。最后，我们将拓展循环神经网络的架构。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey5dfbGMdC9A"
      },
      "source": [
        "**6.1 语言模型 language model**\n",
        "\n",
        "语言模型（language model）是自然语言处理的重要技术。自然语言处理中最常见的数据是文本数据。我们可以把一段自然语言文本看作一段离散的时间序列。假设一段长度为$T$的文本中的词依次为$w_1, w_2, \\ldots, w_T$，那么在离散的时间序列中，$w_t$（$1 \\leq t \\leq T$）可看作在时间步（time step）$t$的输出或标签。给定一个长度为$T$的词的序列$w_1, w_2, \\ldots, w_T$，语言模型将计算该序列的概率：\n",
        "\n",
        "$$P(w_1, w_2, \\ldots, w_T).$$\n",
        "\n",
        "语言模型可用于提升语音识别和机器翻译的性能。例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRL-fif1dTpX"
      },
      "source": [
        "**6.1.1 语言模型的计算**\n",
        "\n",
        "既然语言模型很有用，那该如何计算它呢？假设序列$w_1, w_2, \\ldots, w_T$中的每个词是依次生成的，我们有\n",
        "\n",
        "$$P(w_1, w_2, \\ldots, w_T) = \\prod_{t=1}^T P(w_t \\mid w_1, \\ldots, w_{t-1}).$$\n",
        "\n",
        "例如，一段含有4个词的文本序列的概率\n",
        "\n",
        "$$P(w_1, w_2, w_3, w_4) = P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_1, w_2, w_3).$$\n",
        "\n",
        "为了计算语言模型，我们需要计算词的概率，以及一个词在给定前几个词的情况下的条件概率，即语言模型参数。设训练数据集为一个大型文本语料库，如维基百科的所有条目。词的概率可以通过该词在训练数据集中的相对词频来计算。例如，$P(w_1)$可以计算为$w_1$在训练数据集中的词频（词出现的次数）与训练数据集的总词数之比。因此，根据条件概率定义，一个词在给定前几个词的情况下的条件概率也可以通过训练数据集中的相对词频计算。例如，$P(w_2 \\mid w_1)$可以计算为$w_1, w_2$两词相邻的频率与$w_1$词频的比值，因为该比值即$P(w_1, w_2)$与$P(w_1)$之比；而$P(w_3 \\mid w_1, w_2)$同理可以计算为$w_1$、$w_2$和$w_3$三词相邻的频率与$w_1$和$w_2$两词相邻的频率的比值。以此类推。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyYWK72YfEMj"
      },
      "source": [
        "**6.1.2 n元语法**\n",
        "\n",
        "当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$元语法通过马尔可夫假设（虽然并不一定成立）简化了语言模型的计算。这里的马尔可夫假设是指一个词的出现只与前面$n$个词相关，即$n$阶马尔可夫链（Markov chain of order $n$）。如果$n=1$，那么有$P(w_3 \\mid w_1, w_2) = P(w_3 \\mid w_2)$。如果基于$n-1$阶马尔可夫链，我们可以将语言模型改写为\n",
        "\n",
        "$$P(w_1, w_2, \\ldots, w_T) \\approx \\prod_{t=1}^T P(w_t \\mid w_{t-(n-1)}, \\ldots, w_{t-1}) .$$\n",
        "\n",
        "以上也叫$n$元语法（$n$-grams）。它是基于$n - 1$阶马尔可夫链的概率语言模型。当$n$分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列$w_1, w_2, w_3, w_4$在一元语法、二元语法和三元语法中的概率分别为\n",
        "\n",
        "$$ \\begin{aligned} P(w_1, w_2, w_3, w_4) &= P(w_1) P(w_2) P(w_3) P(w_4) ,\\ P(w_1, w_2, w_3, w_4) &= P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_2) P(w_4 \\mid w_3) ,\\ P(w_1, w_2, w_3, w_4) &= P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_2, w_3) . \\end{aligned} $$\n",
        "\n",
        "当$n$较小时，$n$元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当$n$较大时，$n$元语法需要计算并存储大量的词频和多词相邻频率。\n",
        "\n",
        "那么，有没有方法在语言模型中更好地平衡以上这两点呢？我们将在本章探究这样的方法。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_vjxuHDhYks"
      },
      "source": [
        "**6.2 RNN 循环神经网络 Recurrent Neural Network**\n",
        "\n",
        "上一节介绍的 n 元语法中，时间步 t 的词 wt 基于前面所有词的条件概率只考虑了最近时间步的 n−1 个词。如果要考虑比 t−(n−1) 更早时间步的词对 wt 的可能影响，我们需要增大 n 。但这样模型参数的数量将随之呈指数级增长（可参考上一节的练习）。\n",
        "\n",
        "本节将介绍循环神经网络。它并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。首先我们回忆一下前面介绍过的多层感知机，然后描述如何添加隐藏状态来将它变成循环神经网络。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TnMH6tAtrIz"
      },
      "source": [
        "**6.2.1 不含隐藏状态的神经网络**\n",
        "\n",
        "让我们考虑一个含单隐藏层的多层感知机。给定样本数为$n$、输入个数（特征数或特征向量维度）为$d$的小批量数据样本$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$。设隐藏层的激活函数为$\\phi$，那么隐藏层的输出$\\boldsymbol{H} \\in \\mathbb{R}^{n \\times h}$计算为\n",
        "\n",
        "$$\\boldsymbol{H} = \\phi(\\boldsymbol{X} \\boldsymbol{W}_{xh} + \\boldsymbol{b}_h),$$\n",
        "\n",
        "其中隐藏层权重参数$\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$，隐藏层偏差参数 $\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$，$h$为隐藏单元个数。上式相加的两项形状不同，因此将按照广播机制相加。把隐藏变量$\\boldsymbol{H}$作为输出层的输入，且设输出个数为$q$（如分类问题中的类别数），输出层的输出为\n",
        "\n",
        "$$\\boldsymbol{O} = \\boldsymbol{H} \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q,$$\n",
        "\n",
        "其中输出变量$\\boldsymbol{O} \\in \\mathbb{R}^{n \\times q}$, 输出层权重参数$\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{h \\times q}$, 输出层偏差参数$\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$。如果是分类问题，我们可以使用$\\text{softmax}(\\boldsymbol{O})$来计算输出类别的概率分布。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qfZoOTjuehi"
      },
      "source": [
        "**6.2.2 含隐藏状态的循环神经网络**\n",
        "\n",
        "现在我们考虑输入数据存在时间相关性的情况。假设$\\boldsymbol{X}t \\in \\mathbb{R}^{n \\times d}$是序列中时间步$t$的小批量输入，$\\boldsymbol{H}t \\in \\mathbb{R}^{n \\times h}$是该时间步的隐藏变量。与多层感知机不同的是，这里我们保存上一时间步的隐藏变量$\\boldsymbol{H}{t-1}$，并引入一个新的权重参数$\\boldsymbol{W}{hh} \\in \\mathbb{R}^{h \\times h}$，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，时间步$t$的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：\n",
        "\n",
        "$$\\boldsymbol{H}t = \\phi(\\boldsymbol{X}t \\boldsymbol{W}{xh} + \\boldsymbol{H}{t-1} \\boldsymbol{W}_{hh} + \\boldsymbol{b}_h).$$\n",
        "\n",
        "与多层感知机相比，我们在这里添加了$\\boldsymbol{H}{t-1} \\boldsymbol{W}{hh}$一项。由上式中相邻时间步的隐藏变量$\\boldsymbol{H}t$和$\\boldsymbol{H}{t-1}$之间的关系可知，这里的隐藏变量能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。因此，该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。使用循环计算的网络即循环神经网络（recurrent neural network）。\n",
        "\n",
        "循环神经网络有很多种不同的构造方法。含上式所定义的隐藏状态的循环神经网络是极为常见的一种。若无特别说明，本章中的循环神经网络均基于上式中隐藏状态的循环计算。在时间步$t$，输出层的输出和多层感知机中的计算类似：\n",
        "\n",
        "$$\\boldsymbol{O}_t = \\boldsymbol{H}t \\boldsymbol{W}{hq} + \\boldsymbol{b}_q.$$\n",
        "\n",
        "循环神经网络的参数包括隐藏层的权重$\\boldsymbol{W}{xh} \\in \\mathbb{R}^{d \\times h}$、$\\boldsymbol{W}{hh} \\in \\mathbb{R}^{h \\times h}$和偏差 $\\boldsymbol{b}h \\in \\mathbb{R}^{1 \\times h}$，以及输出层的权重$\\boldsymbol{W}{hq} \\in \\mathbb{R}^{h \\times q}$和偏差$\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$。值得一提的是，即便在不同时间步，循环神经网络也始终使用这些模型参数。因此，循环神经网络模型参数的数量不随时间步的增加而增长。\n",
        "\n",
        "图6.1展示了循环神经网络在3个相邻时间步的计算逻辑。在时间步$t$，隐藏状态的计算可以看成是将输入$\\boldsymbol{X}t$和前一时间步隐藏状态$\\boldsymbol{H}{t-1}$连结后输入一个激活函数为$\\phi$的全连接层。该全连接层的输出就是当前时间步的隐藏状态$\\boldsymbol{H}t$，且模型参数为$\\boldsymbol{W}{xh}$与$\\boldsymbol{W}{hh}$的连结，偏差为$\\boldsymbol{b}_h$。当前时间步$t$的隐藏状态$\\boldsymbol{H}_t$将参与下一个时间步$t+1$的隐藏状态$\\boldsymbol{H}{t+1}$的计算，并输入到当前时间步的全连接输出层。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGIxBf701HOC",
        "outputId": "5269665b-5e75-43ae-d077-6b10665f503e"
      },
      "source": [
        "#例子：首先，我们构造矩阵X、W_xh、H和W_hh，它们的形状分别为(3, 1)、(1, 4)、(3, 4)和(4, 4)。将X与W_xh、H与W_hh分别相乘，再把两个乘法运算的结果相加，得到形状为(3, 4)的矩阵。\n",
        "import torch\n",
        "X, W_xh = torch.randn(3, 1), torch.randn(1, 4)\n",
        "H, W_hh = torch.randn(3, 4), torch.randn(4, 4)\n",
        "output1 = torch.matmul(X, W_xh) + torch.matmul(H, W_hh)\n",
        "print('X:', X)\n",
        "print('W_xh:', W_xh)\n",
        "print('H:', H)\n",
        "print('W_hh:', W_hh)\n",
        "print(output1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: tensor([[ 0.4141],\n",
            "        [-0.5145],\n",
            "        [ 0.4771]])\n",
            "W_xh: tensor([[ 0.1217, -1.1432, -1.4559, -0.9925]])\n",
            "H: tensor([[ 0.7436,  0.8041,  0.3615,  0.1965],\n",
            "        [-0.0587, -0.0195, -0.9182,  0.4399],\n",
            "        [-0.4428,  1.1989, -0.1678,  1.4839]])\n",
            "W_hh: tensor([[-0.4320, -0.8420,  0.7424,  0.7425],\n",
            "        [ 0.7676,  0.6388,  0.3117, -0.8685],\n",
            "        [ 1.0672,  1.1686, -0.6627,  0.5157],\n",
            "        [-1.6162, -1.4470, -0.0504, -1.0152]])\n",
            "tensor([[ 0.4146, -0.4477, -0.0497, -0.5702],\n",
            "        [-1.7430, -1.0844,  1.2857, -0.4361],\n",
            "        [-1.4077, -1.7499, -0.6132, -3.4364]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOI_qxfR03aa"
      },
      "source": [
        "将矩阵X和H按列（维度1）连结，连结后的矩阵形状为(3, 5)。可见，连结后矩阵在维度1的长度为矩阵X和H在维度1的长度之和（$1+4$）。然后，将矩阵W_xh和W_hh按行（维度0）连结，连结后的矩阵形状为(5, 4)。最后将两个连结后的矩阵相乘，得到与上面代码输出相同的形状为(3, 4)的矩阵。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMJBesA03Gjg",
        "outputId": "21e034e7-4e8b-4c1f-f5b4-23ae1b0011ae"
      },
      "source": [
        "output2 = torch.matmul(torch.cat((X, H), dim=1), torch.cat((W_xh, W_hh), dim=0))\n",
        "print(output2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.4146, -0.4477, -0.0497, -0.5702],\n",
            "        [-1.7430, -1.0844,  1.2857, -0.4361],\n",
            "        [-1.4077, -1.7499, -0.6132, -3.4364]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SviUgn-r3fb2"
      },
      "source": [
        "**6.2.3 应用：基于字符级循环神经网络的语言模型**\n",
        "\n",
        "最后我们介绍如何应用循环神经网络来构建一个语言模型。设小批量中样本数为1，文本序列为“想”“要”“有”“直”“升”“机”。图6.2演示了如何使用循环神经网络基于当前和过去的字符来预测下一个字符。在训练时，我们对每个时间步的输出层输出使用softmax运算，然后使用交叉熵损失函数来计算它与标签的误差。在图6.2中，由于隐藏层中隐藏状态的循环计算，时间步3的输出$\\boldsymbol{O}_3$取决于文本序列“想”“要”“有”。 由于训练数据中该序列的下一个词为“直”，时间步3的损失将取决于该时间步基于序列“想”“要”“有”生成下一个词的概率分布与该时间步的标签“直”。因为每个输入词是一个字符，因此这个模型被称为字符级循环神经网络（character-level recurrent neural network）。因为不同字符的个数远小于不同词的个数（对于英文尤其如此），所以字符级循环神经网络的计算通常更加简单。在接下来的几节里，我们将介绍它的具体实现。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9srUXBzF_oSk"
      },
      "source": [
        "**6.3 语言模型数据集**\n",
        "\n",
        "本节将介绍如何预处理一个语言模型数据集，并将其转换成字符级循环神经网络所需要的输入格式。为此，我们收集了周杰伦从第一张专辑《Jay》到第十张专辑《跨时代》中的歌词，并在后面几节里应用循环神经网络来训练一个语言模型。当模型训练好后，我们就可以用这个模型来创作歌词。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMmmBGnr_32X"
      },
      "source": [
        "**6.3.1 读取数据集**\n",
        "\n",
        "首先读取这个数据集，看看前40个字符是什么样的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS-9CbQHET-g"
      },
      "source": [
        "这个数据集有6万多个字符。为了打印方便，我们把换行符替换成空格，然后仅使用前1万个字符来训练模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flgLcCkZA-q4"
      },
      "source": [
        "**6.3.2 建立字符索引**\n",
        "\n",
        "我们将每个字符映射成一个从0开始的连续整数，又称索引，来方便之后的数据处理。为了得到索引，我们将数据集里所有不同字符取出来，然后将其逐一映射到索引来构造词典。接着，打印vocab_size，即词典中不同字符的个数，又称词典大小。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msTWmJT-FFXQ"
      },
      "source": [
        "**6.3.3 时序数字采样**\n",
        "\n",
        "在训练中我们需要每次随机读取小批量样本和标签。与之前章节的实验数据不同的是，时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4rqfME2FZZc"
      },
      "source": [
        "**6.3.3.1 随机采样**\n",
        "\n",
        "下面的代码每次从数据里随机采样一个小批量。其中批量大小batch_size指每个小批量的样本数，num_steps为每个样本所包含的时间步数。 在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相毗邻。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。在训练模型时，每次随机采样前都需要重新初始化隐藏状态。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kae9_uhTIy6N"
      },
      "source": [
        "**6.3.3.2 相邻采样**\n",
        "\n",
        "除对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。这对实现循环神经网络造成了两方面影响：一方面， 在训练模型时，我们只需在每一个迭代周期开始时初始化隐藏状态；另一方面，当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。 为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。我们将在下一节（循环神经网络的从零开始实现）的实现中了解这种处理方式。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHJIx5NzJ-hp"
      },
      "source": [
        "**6.4 循环神经网络从零开始**\n",
        "\n",
        "在本节中，我们将从零开始实现一个基于字符级循环神经网络的语言模型，并在周杰伦专辑歌词数据集上训练一个模型来进行歌词创作。首先，我们读取周杰伦专辑歌词数据集："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4nNqBx1M6QO"
      },
      "source": [
        "**6.4.1 one-hot向量**\n",
        "\n",
        "为了将词表示成向量输入到神经网络，一个简单的办法是使用one-hot向量。假设词典中不同字符的数量为$N$（即词典大小vocab_size），每个字符已经同一个从0到$N-1$的连续整数值索引一一对应。如果一个字符的索引是整数$i$, 那么我们创建一个全0的长为$N$的向量，并将其位置为$i$的元素设成1。该向量就是对原字符的one-hot向量。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。\n",
        "\n",
        "我们每次采样的小批量的形状是(批量大小, 时间步数)。下面的函数将这样的小批量变换成数个可以输入进网络的形状为(批量大小, 词典大小)的矩阵，矩阵个数等于时间步数。也就是说，时间步$t$的输入为$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$，其中$n$为批量大小，$d$为输入个数，即one-hot向量长度（词典大小）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anH2l71oNEGN"
      },
      "source": [
        "**6.4.2 初始化模型参数**\n",
        "\n",
        "我们初始化模型参数。隐藏单元个数 num_hiddens是一个超参数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wczI-VNR0uE"
      },
      "source": [
        "**6.4.3 定义模型**\n",
        "\n",
        "我们根据循环神经网络的计算表达式实现该模型。首先定义init_rnn_state函数来返回初始化的隐藏状态。它返回由一个形状为(批量大小, 隐藏单元个数)的值为0的NDArray组成的元组。使用元组是为了更便于处理隐藏状态含有多个NDArray的情况。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbQnF_ooSouy"
      },
      "source": [
        "**6.4.4 定义预测函数**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttrm5TXKSwAw"
      },
      "source": [
        "**6.4.5 裁剪梯度**\n",
        "\n",
        "循环神经网络中较容易出现梯度衰减或梯度爆炸。我们会在6.6节（通过时间反向传播）中解释原因。为了应对梯度爆炸，我们可以裁剪梯度（clip gradient）。假设我们把所有模型参数梯度的元素拼接成一个向量 $\\boldsymbol{g}$，并设裁剪的阈值是$\\theta$。裁剪后的梯度\n",
        "\n",
        "$$ \\min\\left(\\frac{\\theta}{|\\boldsymbol{g}|}, 1\\right)\\boldsymbol{g}$$\n",
        "\n",
        "的$L_2$范数不超过$\\theta$。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQbKQE20Wmxy"
      },
      "source": [
        "**6.4.6 困惑度**\n",
        "\n",
        "我们通常使用困惑度（perplexity）来评价语言模型的好坏。回忆一下3.4节（softmax回归）中交叉熵损失函数的定义。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，\n",
        "\n",
        "最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；\n",
        "\n",
        "最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；\n",
        "\n",
        "基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。\n",
        "\n",
        "显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典大小vocab_size。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RinD7eCaaSf-"
      },
      "source": [
        "**6.4.7 定义模型训练函数**\n",
        "\n",
        "跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：\n",
        "\n",
        "使用困惑度评价模型。\n",
        "\n",
        "在迭代模型参数前裁剪梯度。\n",
        "\n",
        "对时序数据采用不同采样方法将导致隐藏状态初始化的不同。相关讨论可参考6.3节（语言模型数据集（周杰伦专辑歌词））。\n",
        "\n",
        "另外，考虑到后面将介绍的其他循环神经网络，为了更通用，这里的函数实现更长一些。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coLjO5fhbUO0"
      },
      "source": [
        "**6.4.8 训练并创作歌词**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1jPAzPx8uyJ"
      },
      "source": [
        "**6.5 RNN作词RNN简洁实现**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWY_FwRj86x7",
        "outputId": "5bbd999e-5176-4547-e713-ed208752f275"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks')\n",
        "#sys.path.append(\"..\") \n",
        "import d2lzh_pytorch as d2l\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TByk2HGj9NSg"
      },
      "source": [
        "#(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua5d1hLh-LBf"
      },
      "source": [
        "**6.5.1 定义模型**\n",
        "\n",
        "PyTorch中的nn模块提供了循环神经网络的实现。下面构造一个含单隐藏层、隐藏单元个数为256的循环神经网络层rnn_layer。\n",
        "\n",
        "与上一节中实现的循环神经网络不同，这里rnn_layer的输入形状为(时间步数, 批量大小, 输入个数)。其中输入个数即one-hot向量长度（词典大小）。此外，rnn_layer作为nn.RNN实例，在前向计算后会分别返回输出和隐藏状态h，其中输出指的是隐藏层在各个时间步上计算并输出的隐藏状态，它们通常作为后续输出层的输入。需要强调的是，该“输出”本身并不涉及输出层计算，形状为(时间步数, 批量大小, 隐藏单元个数)。而nn.RNN实例在前向计算返回的隐藏状态指的是隐藏层在最后时间步的隐藏状态：当隐藏层有多层时，每一层的隐藏状态都会记录在该变量中；对于像长短期记忆（LSTM），隐藏状态是一个元组(h, c)，即hidden state和cell state。我们会在本章的后面介绍长短期记忆和深度循环神经网络。关于循环神经网络（以LSTM为例）的输出，可以参考下图"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnY9xks7_7pZ"
      },
      "source": [
        "**6.5.2 训练模型**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BNmDnR1AKHs"
      },
      "source": [
        "**6.6 通过时间反向传播**\n",
        "\n",
        "在前面两节中，如果不裁剪梯度，模型将无法正常训练。为了深刻理解这一现象，本节将介绍循环神经网络中梯度的计算和存储方法，即通过时间反向传播（back-propagation through time）。\n",
        "\n",
        "我们在3.14节（正向传播、反向传播和计算图）中介绍了神经网络中梯度计算与存储的一般思路，并强调正向传播和反向传播相互依赖。正向传播在循环神经网络中比较直观，而通过时间反向传播其实是反向传播在循环神经网络中的具体应用。我们需要将循环神经网络按时间步展开，从而得到模型变量和参数之间的依赖关系，并依据链式法则应用反向传播计算并存储梯度。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFaIHgFZBXcD"
      },
      "source": [
        "**6.6.1 定义模型**\n",
        "\n",
        "简单起见，我们考虑一个无偏差项的循环神经网络，且激活函数为恒等映射（$\\phi(x)=x$）。设时间步 $t$ 的输入为单样本 $\\boldsymbol{x}_t \\in \\mathbb{R}^d$，标签为 $y_t$，那么隐藏状态 $\\boldsymbol{h}_t \\in \\mathbb{R}^h$的计算表达式为\n",
        "\n",
        "$$ \\boldsymbol{h}t = \\boldsymbol{W}{hx} \\boldsymbol{x}t + \\boldsymbol{W}{hh} \\boldsymbol{h}_{t-1}, $$\n",
        "\n",
        "其中$\\boldsymbol{W}{hx} \\in \\mathbb{R}^{h \\times d}$和$\\boldsymbol{W}{hh} \\in \\mathbb{R}^{h \\times h}$是隐藏层权重参数。设输出层权重参数$\\boldsymbol{W}_{qh} \\in \\mathbb{R}^{q \\times h}$，时间步$t$的输出层变量$\\boldsymbol{o}_t \\in \\mathbb{R}^q$计算为\n",
        "\n",
        "$$ \\boldsymbol{o}t = \\boldsymbol{W}{qh} \\boldsymbol{h}_{t}. $$\n",
        "\n",
        "设时间步$t$的损失为$\\ell(\\boldsymbol{o}_t, y_t)$。时间步数为$T$的损失函数$L$定义为\n",
        "\n",
        "$$ L = \\frac{1}{T} \\sum_{t=1}^T \\ell (\\boldsymbol{o}_t, y_t). $$\n",
        "\n",
        "我们将$L$称为有关给定时间步的数据样本的目标函数，并在本节后续讨论中简称为目标函数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpxDgXjsBdED"
      },
      "source": [
        "**6.6.2 模型计算图**\n",
        "\n",
        "为了可视化循环神经网络中模型变量和参数在计算中的依赖关系，我们可以绘制模型计算图，如图6.3所示。例如，时间步3的隐藏状态$\\boldsymbol{h}_3$的计算依赖模型参数$\\boldsymbol{W}_{hx}$、$\\boldsymbol{W}_{hh}$、上一时间步隐藏状态$\\boldsymbol{h}_2$以及当前时间步输入$\\boldsymbol{x}_3$。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP7aN7GQh7w6"
      },
      "source": [
        "**6.3.3 方法**\n",
        "\n",
        "刚刚提到，图6.3中的模型的参数是 $\\boldsymbol{W}_{hx}$, $\\boldsymbol{W}_{hh}$ 和 $\\boldsymbol{W}_{qh}$。与3.14节（正向传播、反向传播和计算图）中的类似，训练模型通常需要模型参数的梯度$\\partial L/\\partial \\boldsymbol{W}_{hx}$、$\\partial L/\\partial \\boldsymbol{W}_{hh}$和$\\partial L/\\partial \\boldsymbol{W}_{qh}$。\n",
        "根据图6.3中的依赖关系，我们可以按照其中箭头所指的反方向依次计算并存储梯度。为了表述方便，我们依然采用3.14节中表达链式法则的运算符prod。\n",
        "\n",
        "首先，目标函数有关各时间步输出层变量的梯度$\\partial L/\\partial \\boldsymbol{o}_t \\in \\mathbb{R}^q$很容易计算：\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\boldsymbol{o}_t} =  \\frac{\\partial \\ell (\\boldsymbol{o}_t, y_t)}{T \\cdot \\partial \\boldsymbol{o}_t}.$$\n",
        "\n",
        "下面，我们可以计算目标函数有关模型参数$\\boldsymbol{W}_{qh}$的梯度$\\partial L/\\partial \\boldsymbol{W}_{qh} \\in \\mathbb{R}^{q \\times h}$。根据图6.3，$L$通过$\\boldsymbol{o}_1, \\ldots, \\boldsymbol{o}_T$依赖$\\boldsymbol{W}_{qh}$。依据链式法则，\n",
        "$$\\frac{\\partial L}{\\partial \\boldsymbol{W}_{qh}} = \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_t}, \\frac{\\partial \\boldsymbol{o}_t}{\\partial \\boldsymbol{W}_{qh}}\\right) = \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\boldsymbol{o}_t} \\boldsymbol{h}_t^\\top.$$\n",
        "\n",
        "其次，我们注意到隐藏状态之间也存在依赖关系。\n",
        "在图6.3中，$L$只通过$\\boldsymbol{o}_T$依赖最终时间步$T$的隐藏状态$\\boldsymbol{h}_T$。因此，我们先计算目标函数有关最终时间步隐藏状态的梯度$\\partial L/\\partial \\boldsymbol{h}_T \\in \\mathbb{R}^h$。依据链式法则，我们得到\n",
        "$$\\frac{\\partial L}{\\partial \\boldsymbol{h}_T} = \\text{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{o}_T}, \\frac{\\partial \\boldsymbol{o}_T}{\\partial \\boldsymbol{h}_T} \\right) = \\boldsymbol{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\boldsymbol{o}_T}.$$\n",
        "\n",
        "接下来对于时间步$t < T$, 在图6.3中，$L$通过$\\boldsymbol{h}_{t+1}$和$\\boldsymbol{o}_t$依赖$\\boldsymbol{h}_t$。依据链式法则，\n",
        "目标函数有关时间步$t < T$的隐藏状态的梯度$\\partial L/\\partial \\boldsymbol{h}_t \\in \\mathbb{R}^h$需要按照时间步从大到小依次计算：\n",
        "$$\\frac{\\partial L}{\\partial \\boldsymbol{h}_t} = \\text{prod} (\\frac{\\partial L}{\\partial \\boldsymbol{h}_{t+1}}, \\frac{\\partial \\boldsymbol{h}_{t+1}}{\\partial \\boldsymbol{h}_t}) + \\text{prod} (\\frac{\\partial L}{\\partial \\boldsymbol{o}_t}, \\frac{\\partial \\boldsymbol{o}_t}{\\partial \\boldsymbol{h}_t} ) = \\boldsymbol{W}_{hh}^\\top \\frac{\\partial L}{\\partial \\boldsymbol{h}_{t+1}} + \\boldsymbol{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\boldsymbol{o}_t}$$\n",
        "\n",
        "将上面的递归公式展开，对任意时间步$1 \\leq t \\leq T$，我们可以得到目标函数有关隐藏状态梯度的通项公式\n",
        "$$\\frac{\\partial L}{\\partial \\boldsymbol{h}_t} = \\sum_{i=t}^T {\\left(\\boldsymbol{W}_{hh}^\\top\\right)}^{T-i} \\boldsymbol{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\boldsymbol{o}_{T+t-i}}.$$\n",
        "\n",
        "由上式中的指数项可见，当时间步数 $T$ 较大或者时间步 $t$ 较小时，目标函数有关隐藏状态的梯度较容易出现衰减和爆炸。这也会影响其他包含$\\partial L / \\partial \\boldsymbol{h}_t$项的梯度，例如隐藏层中模型参数的梯度$\\partial L / \\partial \\boldsymbol{W}_{hx} \\in \\mathbb{R}^{h \\times d}$和$\\partial L / \\partial \\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$。\n",
        "在图6.3中，$L$通过$\\boldsymbol{h}_1, \\ldots, \\boldsymbol{h}_T$依赖这些模型参数。\n",
        "依据链式法则，我们有\n",
        "$$\\begin{aligned}\\frac{\\partial L}{\\partial \\boldsymbol{W}_{hx}} &= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{h}_t}, \\frac{\\partial \\boldsymbol{h}_t}{\\partial \\boldsymbol{W}_{hx}}\\right) = \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\boldsymbol{h}_t} \\boldsymbol{x}_t^\\top,\\\\\\frac{\\partial L}{\\partial \\boldsymbol{W}_{hh}} &= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\boldsymbol{h}_t}, \\frac{\\partial \\boldsymbol{h}_t}{\\partial \\boldsymbol{W}_{hh}}\\right) = \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\boldsymbol{h}_t} \\boldsymbol{h}_{t-1}^\\top.\\end{aligned}$$\n",
        "\n",
        "我们已在3.14节里解释过，每次迭代中，我们在依次计算完以上各个梯度后，会将它们存储起来，从而避免重复计算。例如，由于隐藏状态梯度$\\partial L/\\partial \\boldsymbol{h}_t$被计算和存储，之后的模型参数梯度$\\partial L/\\partial  \\boldsymbol{W}_{hx}$和$\\partial L/\\partial \\boldsymbol{W}_{hh}$的计算可以直接读取$\\partial L/\\partial \\boldsymbol{h}_t$的值，而无须重复计算它们。此外，反向传播中的梯度计算可能会依赖变量的当前值。它们正是通过正向传播计算出来的。\n",
        "举例来说，参数梯度$\\partial L/\\partial \\boldsymbol{W}_{hh}$的计算需要依赖隐藏状态在时间步$t = 0, \\ldots, T-1$的当前值$\\boldsymbol{h}_t$（$\\boldsymbol{h}_0$是初始化得到的）。这些值是通过从输入层到输出层的正向传播计算并存储得到的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ce5Ysxi3lq"
      },
      "source": [
        "**6.7 门控循环单元**\n",
        "\n",
        "我们发现，当时间步数较大或者时间步较小时，循环神经网络的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。\n",
        "\n",
        "门控循环神经网络（gated recurrent neural network）的提出，正是为了更好地捕捉时间序列中时间步距离较大的依赖关系。它通过可以学习的门来控制信息的流动。其中，门控循环单元（gated recurrent unit，GRU）是一种常用的门控循环神经网络 [1, 2]。另一种常用的门控循环神经网络则将在下一节中介绍。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4-7n17jaQVF"
      },
      "source": [
        "**6.7.1 GRU**\n",
        "\n",
        "下面将介绍门控循环单元的设计。它引入了重置门（reset gate）和更新门（update gate）的概念，从而修改了循环神经网络中隐藏状态的计算方式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KNnYiaPaY61"
      },
      "source": [
        "**6.7.1.1 重置门和更新门**\n",
        "\n",
        "如图6.4所示，门控循环单元中的重置门和更新门的输入均为当前时间步输入$\\boldsymbol{X}_t$与上一时间步隐藏状态$\\boldsymbol{H}_{t-1}$，输出由激活函数为sigmoid函数的全连接层计算得到。\n",
        "\n",
        "具体来说，假设隐藏单元个数为$h$，给定时间步$t$的小批量输入$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$（样本数为$n$，输入个数为$d$）和上一时间步隐藏状态$\\boldsymbol{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$。重置门$\\boldsymbol{R}_t \\in \\mathbb{R}^{n \\times h}$和更新门$\\boldsymbol{Z}_t \\in \\mathbb{R}^{n \\times h}$的计算如下：\n",
        "$$\\begin{aligned}\\boldsymbol{R}_t = \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xr} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hr} + \\boldsymbol{b}_r),\\\\\\boldsymbol{Z}_t = \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xz} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hz} + \\boldsymbol{b}_z),\\end{aligned}$$\n",
        "其中$\\boldsymbol{W}_{xr}, \\boldsymbol{W}_{xz} \\in \\mathbb{R}^{d \\times h}$和$\\boldsymbol{W}_{hr}, \\boldsymbol{W}_{hz} \\in \\mathbb{R}^{h \\times h}$是权重参数，$\\boldsymbol{b}_r, \\boldsymbol{b}_z \\in \\mathbb{R}^{1 \\times h}$是偏差参数。3.8节（多层感知机）节中介绍过，sigmoid函数可以将元素的值变换到0和1之间。因此，重置门$\\boldsymbol{R}_t$和更新门$\\boldsymbol{Z}_t$中每个元素的值域都是$[0, 1]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnDlw_r0avds"
      },
      "source": [
        "**6.7.1.2 候选隐藏状态**\n",
        "\n",
        "接下来，门控循环单元将计算候选隐藏状态来辅助稍后的隐藏状态计算。如图6.5所示，我们将当前时间步重置门的输出与上一时间步隐藏状态做按元素乘法（符号为$\\odot$）。如果重置门中元素值接近0，那么意味着重置对应隐藏状态元素为0，即丢弃上一时间步的隐藏状态。如果元素值接近1，那么表示保留上一时间步的隐藏状态。然后，将按元素乘法的结果与当前时间步的输入连结，再通过含激活函数tanh的全连接层计算出候选隐藏状态，其所有元素的值域为$[-1, 1]$。\n",
        "\n",
        "具体来说，时间步$t$的候选隐藏状态$\\tilde{\\boldsymbol{H}}_t \\in \\mathbb{R}^{n \\times h}$的计算为\n",
        "\n",
        "$$\\tilde{\\boldsymbol{H}}_t = \\text{tanh}(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\left(\\boldsymbol{R}_t \\odot \\boldsymbol{H}_{t-1}\\right) \\boldsymbol{W}_{hh} + \\boldsymbol{b}_h),$$\n",
        "\n",
        "其中$\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$和$\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$是权重参数，$\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$是偏差参数。从上面这个公式可以看出，重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态。而上一时间步的隐藏状态可能包含了时间序列截至上一时间步的全部历史信息。因此，重置门可以用来丢弃与预测无关的历史信息。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cUYc04vdNn3"
      },
      "source": [
        "**6.7.1.3 隐藏状态**\n",
        "\n",
        "最后，时间步$t$的隐藏状态$\\boldsymbol{H}_t \\in \\mathbb{R}^{n \\times h}$的计算使用当前时间步的更新门$\\boldsymbol{Z}_t$来对上一时间步的隐藏状态$\\boldsymbol{H}_{t-1}$和当前时间步的候选隐藏状态$\\tilde{\\boldsymbol{H}}_t$做组合：\n",
        "\n",
        "$$\\boldsymbol{H}_t = \\boldsymbol{Z}_t \\odot \\boldsymbol{H}_{t-1}  + (1 - \\boldsymbol{Z}_t) \\odot \\tilde{\\boldsymbol{H}}_t.$$\n",
        "\n",
        "值得注意的是，更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新，如图6.6所示。假设更新门在时间步$t'$到$t$（$t' < t$）之间一直近似1。那么，在时间步$t'$到$t$之间的输入信息几乎没有流入时间步$t$的隐藏状态$\\boldsymbol{H}_t$。实际上，这可以看作是较早时刻的隐藏状态$\\boldsymbol{H}_{t'-1}$一直通过时间保存并传递至当前时间步$t$。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。\n",
        "\n",
        "我们对门控循环单元的设计稍作总结：\n",
        "\n",
        "* 重置门有助于捕捉时间序列里短期的依赖关系；\n",
        "* 更新门有助于捕捉时间序列里长期的依赖关系。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLR8ys4YjHfR"
      },
      "source": [
        "**6.7.2 实现 读取数据**\n",
        "\n",
        "为了实现并展示门控循环单元，下面依然使用周杰伦歌词数据集来训练模型作词。这里除门控循环单元以外的实现已在6.2节（循环神经网络）中介绍过。以下为读取数据集部分。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Op-oMCYLCE"
      },
      "source": [
        "#导入包和模块\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\") \n",
        "import d2lzh_pytorch as d2l\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ_9Dy7TYejV"
      },
      "source": [
        "#读取数据\n",
        "#(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5TRbMQlZgfU"
      },
      "source": [
        "**6.7.3.1 初始化模型参数**\n",
        "\n",
        "hyperparameter: \n",
        "num_hiddens: hidden units' number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JazhyZA1Z26j"
      },
      "source": [
        "**6.7.3.2 定义模型**\n",
        "\n",
        "下面的代码定义隐藏状态初始化函数`init_gru_state`。同6.4节（循环神经网络的从零开始实现）中定义的`init_rnn_state`函数一样，它返回由一个形状为(批量大小, 隐藏单元个数)的值为0的`Tensor`组成的元组。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXqZwrnuZ_Dr"
      },
      "source": [
        "**6.7.3.3 训练模型并创造歌词**\n",
        "我们在训练模型时只使用相邻采样。设置好超参数后，我们将训练模型并根据前缀“分开”和“不分开”分别创作长度为50个字符的一段歌词。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFwT0j4MaHLz"
      },
      "source": [
        "**6.7.4 简洁实现**\n",
        "\n",
        "直接调用模块中的GRU类即可"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoYoQWpoaXk9"
      },
      "source": [
        "**6.8 长短期记忆（LSTM）**\n",
        "Long short-term memory\n",
        "\n",
        "LSTM是一种常用的门控制循环神经网络"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT4Ik-BJauPP"
      },
      "source": [
        "**6.8.1 长短期记忆**\n",
        "\n",
        "LSTM 中引入了3个门，即输入门（input gate）、遗忘门（forget gate）和输出门（output gate），以及与隐藏状态形状相同的记忆细胞（某些文献把记忆细胞当成一种特殊的隐藏状态），从而记录额外的信息。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svZwpuAlaz4q"
      },
      "source": [
        "**6.8.1.1 输入门、遗忘门、输出门**\n",
        "\n",
        "与门控循环单元中的重置门和更新门一样，如图6.7所示，长短期记忆的门的输入均为当前时间步输入$\\boldsymbol{X}_t$与上一时间步隐藏状态$\\boldsymbol{H}_{t-1}$，输出由激活函数为sigmoid函数的全连接层计算得到。如此一来，这3个门元素的值域均为$[0,1]$。\n",
        "\n",
        "具体来说，假设隐藏单元个数为$h$，给定时间步$t$的小批量输入$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$（样本数为$n$，输入个数为$d$）和上一时间步隐藏状态$\\boldsymbol{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$。\n",
        "时间步$t$的输入门$\\boldsymbol{I}_t \\in \\mathbb{R}^{n \\times h}$、遗忘门$\\boldsymbol{F}_t \\in \\mathbb{R}^{n \\times h}$和输出门$\\boldsymbol{O}_t \\in \\mathbb{R}^{n \\times h}$分别计算如下：\n",
        "$$\\begin{aligned}\n",
        "\\boldsymbol{I}_t &= \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xi} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hi} + \\boldsymbol{b}_i),\\\\\n",
        "\\boldsymbol{F}_t &= \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xf} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hf} + \\boldsymbol{b}_f),\\\\\n",
        "\\boldsymbol{O}_t &= \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xo} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{ho} + \\boldsymbol{b}_o),\n",
        "\\end{aligned}$$\n",
        "其中的$\\boldsymbol{W}_{xi}, \\boldsymbol{W}_{xf}, \\boldsymbol{W}_{xo} \\in \\mathbb{R}^{d \\times h}$和$\\boldsymbol{W}_{hi}, \\boldsymbol{W}_{hf}, \\boldsymbol{W}_{ho} \\in \\mathbb{R}^{h \\times h}$是权重参数，$\\boldsymbol{b}_i, \\boldsymbol{b}_f, \\boldsymbol{b}_o \\in \\mathbb{R}^{1 \\times h}$是偏差参数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA-Uq_BgbUL8"
      },
      "source": [
        "**6.8.1.2 候选记忆细胞**\n",
        "\n",
        "接下来，长短期记忆需要计算候选记忆细胞$\\tilde{\\boldsymbol{C}}_t$。它的计算与上面介绍的3个门类似，但使用了值域在$[-1, 1]$的tanh函数作为激活函数，如图6.8所示。\n",
        "\n",
        "具体来说，时间步$t$的候选记忆细胞$\\tilde{\\boldsymbol{C}}_t \\in \\mathbb{R}^{n \\times h}$的计算为\n",
        "\n",
        "$$\n",
        "\\tilde{\\boldsymbol{C}}_t = \\text{tanh}(\\boldsymbol{X}_t \\boldsymbol{W}_{xc} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hc} + \\boldsymbol{b}_c),\n",
        "$$\n",
        "\n",
        "其中$\\boldsymbol{W}_{xc} \\in \\mathbb{R}^{d \\times h}$和$\\boldsymbol{W}_{hc} \\in \\mathbb{R}^{h \\times h}$是权重参数，$\\boldsymbol{b}_c \\in \\mathbb{R}^{1 \\times h}$是偏差参数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7mXMpAuc-4m"
      },
      "source": [
        "**6.8.1.3 记忆细胞**\n",
        "\n",
        "我们可以通过元素值域在$[0, 1]$的输入门、遗忘门和输出门来控制隐藏状态中信息的流动，这一般也是通过使用按元素乘法（符号为$\\odot$）来实现的。当前时间步记忆细胞$\\boldsymbol{C}_t \\in \\mathbb{R}^{n \\times h}$的计算组合了上一时间步记忆细胞和当前时间步候选记忆细胞的信息，并通过遗忘门和输入门来控制信息的流动：\n",
        "\n",
        "$$\\boldsymbol{C}_t = \\boldsymbol{F}_t \\odot \\boldsymbol{C}_{t-1} + \\boldsymbol{I}_t \\odot \\tilde{\\boldsymbol{C}}_t.$$\n",
        "\n",
        "\n",
        "如图6.9所示，遗忘门控制上一时间步的记忆细胞$\\boldsymbol{C}_{t-1}$中的信息是否传递到当前时间步，而输入门则控制当前时间步的输入$\\boldsymbol{X}_t$通过候选记忆细胞$\\tilde{\\boldsymbol{C}}_t$如何流入当前时间步的记忆细胞。如果遗忘门一直近似1且输入门一直近似0，过去的记忆细胞将一直通过时间保存并传递至当前时间步。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PakL_-kreBAS"
      },
      "source": [
        "**6.8.1.4 隐藏状态**\n",
        "\n",
        "有了记忆细胞以后，接下来我们还可以通过输出门来控制从记忆细胞到隐藏状态$\\boldsymbol{H}_t \\in \\mathbb{R}^{n \\times h}$的信息的流动：\n",
        "\n",
        "$$\\boldsymbol{H}_t = \\boldsymbol{O}_t \\odot \\text{tanh}(\\boldsymbol{C}_t).$$\n",
        "\n",
        "这里的tanh函数确保隐藏状态元素值在-1到1之间。需要注意的是，当输出门近似1时，记忆细胞信息将传递到隐藏状态供输出层使用；当输出门近似0时，记忆细胞信息只自己保留。图6.10展示了长短期记忆中隐藏状态的计算。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC6QsCiBjdKq"
      },
      "source": [
        "**6.9 深度循环神经网络Deep-RNN**\n",
        "\n",
        "本章到目前为止介绍的循环神经网络只有一个单向的隐藏层，在深度学习应用里，我们通常会用到含有多个隐藏层的循环神经网络，也称作深度循环神经网络。图6.11演示了一个有$L$个隐藏层的深度循环神经网络，每个隐藏状态不断传递至当前层的下一时间步和当前时间步的下一层。\n",
        "\n",
        "具体来说，在时间步$t$里，设小批量输入$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$（样本数为$n$，输入个数为$d$），第$\\ell$隐藏层（$\\ell=1,\\ldots,L$）的隐藏状态为$\\boldsymbol{H}_t^{(\\ell)}  \\in \\mathbb{R}^{n \\times h}$（隐藏单元个数为$h$），输出层变量为$\\boldsymbol{O}_t \\in \\mathbb{R}^{n \\times q}$（输出个数为$q$），且隐藏层的激活函数为$\\phi$。第1隐藏层的隐藏状态和之前的计算一样：\n",
        "\n",
        "$$\\boldsymbol{H}_t^{(1)} = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(1)} + \\boldsymbol{H}_{t-1}^{(1)} \\boldsymbol{W}_{hh}^{(1)}  + \\boldsymbol{b}_h^{(1)}),$$\n",
        "\n",
        "\n",
        "其中权重$\\boldsymbol{W}_{xh}^{(1)} \\in \\mathbb{R}^{d \\times h}$、$\\boldsymbol{W}_{hh}^{(1)} \\in \\mathbb{R}^{h \\times h}$和偏差 $\\boldsymbol{b}_h^{(1)} \\in \\mathbb{R}^{1 \\times h}$分别为第1隐藏层的模型参数。\n",
        "\n",
        "当$1 < \\ell \\leq L$时，第$\\ell$隐藏层的隐藏状态的表达式为\n",
        "\n",
        "$$\\boldsymbol{H}_t^{(\\ell)} = \\phi(\\boldsymbol{H}_t^{(\\ell-1)} \\boldsymbol{W}_{xh}^{(\\ell)} + \\boldsymbol{H}_{t-1}^{(\\ell)} \\boldsymbol{W}_{hh}^{(\\ell)}  + \\boldsymbol{b}_h^{(\\ell)}),$$\n",
        "\n",
        "\n",
        "其中权重$\\boldsymbol{W}_{xh}^{(\\ell)} \\in \\mathbb{R}^{h \\times h}$、$\\boldsymbol{W}_{hh}^{(\\ell)} \\in \\mathbb{R}^{h \\times h}$和偏差 $\\boldsymbol{b}_h^{(\\ell)} \\in \\mathbb{R}^{1 \\times h}$分别为第$\\ell$隐藏层的模型参数。\n",
        "\n",
        "最终，输出层的输出只需基于第$L$隐藏层的隐藏状态：\n",
        "\n",
        "$$\\boldsymbol{O}_t = \\boldsymbol{H}_t^{(L)} \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q,$$\n",
        "\n",
        "其中权重$\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{h \\times q}$和偏差$\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$为输出层的模型参数。\n",
        "\n",
        "同多层感知机一样，隐藏层个数$L$和隐藏单元个数$h$都是超参数。此外，如果将隐藏状态的计算换成门控循环单元或者长短期记忆的计算，我们可以得到深度门控循环神经网络。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKnO-oEJnGVj"
      },
      "source": [
        "**6.10 双向循环神经网络**\n",
        "\n",
        "之前介绍的循环神经网络模型都是假设当前时间步是由前面的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。有时候，当前时间步也可能由后面时间步决定。例如，当我们写下一个句子时，可能会根据句子后面的词来修改句子前面的用词。双向循环神经网络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息。图6.12演示了一个含单隐藏层的双向循环神经网络的架构。\n",
        "\n",
        "下面我们来介绍具体的定义。 给定时间步$t$的小批量输入$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$（样本数为$n$，输入个数为$d$）和隐藏层激活函数为$\\phi$。在双向循环神经网络的架构中， 设该时间步正向隐藏状态为$\\overrightarrow{\\boldsymbol{H}}_t \\in \\mathbb{R}^{n \\times h}$（正向隐藏单元个数为$h$）， 反向隐藏状态为$\\overleftarrow{\\boldsymbol{H}}_t \\in \\mathbb{R}^{n \\times h}$（反向隐藏单元个数为$h$）。我们可以分别计算正向隐藏状态和反向隐藏状态：\n",
        "\n",
        "$$\\begin{aligned} \\overrightarrow{\\boldsymbol{H}}t &= \\phi(\\boldsymbol{X}t \\boldsymbol{W}{xh}^{(f)} + \\overrightarrow{\\boldsymbol{H}}{t-1} \\boldsymbol{W}{hh}^{(f)} + \\boldsymbol{b}h^{(f)}),\\ \\overleftarrow{\\boldsymbol{H}}_t &= \\phi(\\boldsymbol{X}_t \\boldsymbol{W}{xh}^{(b)} + \\overleftarrow{\\boldsymbol{H}}{t+1} \\boldsymbol{W}_{hh}^{(b)} + \\boldsymbol{b}_h^{(b)}), \\end{aligned}$$\n",
        "\n",
        "其中权重$\\boldsymbol{W}{xh}^{(f)} \\in \\mathbb{R}^{d \\times h}$、$\\boldsymbol{W}{hh}^{(f)} \\in \\mathbb{R}^{h \\times h}$、$\\boldsymbol{W}{xh}^{(b)} \\in \\mathbb{R}^{d \\times h}$、$\\boldsymbol{W}{hh}^{(b)} \\in \\mathbb{R}^{h \\times h}$和偏差 $\\boldsymbol{b}_h^{(f)} \\in \\mathbb{R}^{1 \\times h}$、$\\boldsymbol{b}_h^{(b)} \\in \\mathbb{R}^{1 \\times h}$均为模型参数。\n",
        "\n",
        "然后我们连结两个方向的隐藏状态$\\overrightarrow{\\boldsymbol{H}}_t$和$\\overleftarrow{\\boldsymbol{H}}_t$来得到隐藏状态$\\boldsymbol{H}_t \\in \\mathbb{R}^{n \\times 2h}$，并将其输入到输出层。输出层计算输出$\\boldsymbol{O}_t \\in \\mathbb{R}^{n \\times q}$（输出个数为$q$）：\n",
        "\n",
        "$$\\boldsymbol{O}_t = \\boldsymbol{H}t \\boldsymbol{W}{hq} + \\boldsymbol{b}_q,$$\n",
        "\n",
        "其中权重$\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{2h \\times q}$和偏差$\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$为输出层的模型参数。不同方向上的隐藏单元个数也可以不同。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkA8y9lHbapD"
      },
      "source": [
        ""
      ]
    }
  ]
}